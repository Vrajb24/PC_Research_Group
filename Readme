# Distributed Deep Learning Communication Optimization

This repository hosts the research, implementations, and benchmarks developed by our group at IIT Kanpur focused on optimizing communication between compute nodes in large-scale high-performance computing (HPC) environments. Our work targets the communication bottlenecks in distributed deep learning training, especially for large-scale models running across multi-node clusters.

## ğŸ”¬ Research Focus

<to be added>

## ğŸ§  Current Projects

<to be added>

## ğŸ“ Repository Structure

```
ğŸ“‚ benchmarks/        # Microbenchmarks for AllReduce, Scatter-Gather, etc.
ğŸ“‚ scripts/           # Job scripts for various HPC systems
ğŸ“‚ experiments/       # Model training experiments and configurations
ğŸ“‚ analysis/          # Profiling logs, traces, and visualization notebooks
ğŸ“‚ docs/              # Presentation slides, papers, and research logs
```

## âš™ï¸ Dependencies

<to be added>

## ğŸš€ Running Benchmarks

```bash
cd benchmarks
bash run_allreduce_benchmark.sh
```

<to be added: usage notes for other scripts>

## ğŸ“Š Results

<to be added>

## ğŸ“ Publications & Reports

<to be added>

## ğŸ¤ Contributing

<to be added>

---

Â© 2025 IIT Kanpur HPC-DL Communication Research Group

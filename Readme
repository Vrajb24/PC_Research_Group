# Distributed Deep Learning Communication Optimization

This repository hosts the research, implementations, and benchmarks developed by our group at IIT Kanpur focused on optimizing communication between compute nodes in large-scale high-performance computing (HPC) environments. Our work targets the communication bottlenecks in distributed deep learning training, especially for large-scale models running across multi-node clusters.

## 🔬 Research Focus

<to be added>

## 🧠 Current Projects

<to be added>

## 📁 Repository Structure

```
📂 benchmarks/        # Microbenchmarks for AllReduce, Scatter-Gather, etc.
📂 scripts/           # Job scripts for various HPC systems
📂 experiments/       # Model training experiments and configurations
📂 analysis/          # Profiling logs, traces, and visualization notebooks
📂 docs/              # Presentation slides, papers, and research logs
```

## ⚙️ Dependencies

<to be added>

## 🚀 Running Benchmarks

```bash
cd benchmarks
bash run_allreduce_benchmark.sh
```

<to be added: usage notes for other scripts>

## 📊 Results

<to be added>

## 📝 Publications & Reports

<to be added>

## 🤝 Contributing

<to be added>

---

© 2025 IIT Kanpur HPC-DL Communication Research Group
